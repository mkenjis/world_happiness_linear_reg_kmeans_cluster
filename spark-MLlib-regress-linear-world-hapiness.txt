
val rdd = sc.textFile("world_happiness/world_happiness_report_2015.csv").map( x => x.split(","))

val rdd1 = rdd.map( x => x.slice(3,x.size).map( x => x.toDouble))

rdd1.first

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = rdd1.map( x => {
   val arr_size = x.size
   val l = x(0)
   val f = x.slice(1,arr_size)
   LabeledPoint(l,Vectors.dense(f))
 })
 
val sets = data.randomSplit(Array(0.8,0.2))
val trainSet = sets(0)
val testSet = sets(1)

trainSet.cache

---- MLlib logistic regression --------------

import org.apache.spark.mllib.regression.LinearRegressionWithSGD
val model = LinearRegressionWithSGD.train(trainSet, 200, 1.0)

val alg = new LinearRegressionWithSGD()
alg.setIntercept(true)
alg.optimizer.setNumIterations(200)
alg.optimizer.setStepSize(1.0)
val model = alg.run(trainSet)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res6: Array[(Double, Double)] = Array((7.128668661003051,7.527), (7.109346649004852,7.278), (6.7883147632169045,6.946), (6.603731566439976,6.798), (6.552075365039732,6.574), (6.067416094800416,5.987), (5.877603749019901,5.824), (5.970908343780179,5.759), (5.780423776848055,5.689), (5.722046825576282,5.605))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  // 0.2146084670916074
validMetrics.meanSquaredError  // 0.04605679414740955

---- MLlib decision tree regression --------------

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int]()  // no categorical features in dataset
val impurity = "variance"
val maxDepth = 5
val maxBins = 32

val model = DecisionTree.trainRegressor(trainSet, categoricalFeaturesInfo, impurity, maxDepth, maxBins)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(10)
res9: Array[(Double, Double)] = Array((7.407888888888891,7.527), (6.27,7.278), (7.0322000000000005,6.946), (5.180923076923078,6.798), (6.8965,6.574), (5.474,5.987), (6.0306000000000015,5.824), (5.391500000000001,5.759), (5.180923076923078,5.689), (5.8188666666666675,5.605))

import org.apache.spark.mllib.evaluation.RegressionMetrics
val validMetrics = new RegressionMetrics(validPredicts)
validMetrics.rootMeanSquaredError  // 0.6863887053994453
validMetrics.meanSquaredError  // 0.4711294548999265

-------------------------

val vect = rdd1.map( x => Vectors.dense(x(1),x(2),x(3),x(4),x(5),x(6),x(7),x(8),x(0)))

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix
val matrix = new RowMatrix(vect)
val colstats = matrix.computeColumnSummaryStatistics

colstats.min
res16: org.apache.spark.mllib.linalg.Vector = [0.01848,0.0,0.0,0.0,0.0,0.0,0.0,0.32858,2.839]

colstats.max
res17: org.apache.spark.mllib.linalg.Vector = [0.13693,1.69042,1.40223,1.02525,0.66973,0.55191,0.79588,3.60214,7.587]

colstats.mean
res19: org.apache.spark.mllib.linalg.Vector = [0.04788474683544302,0.8461372151898745,0.9910459493670879,0.6302593670886076,0.4286149367088608,0.14342183544303796,0.23729550632911395,2.098976772151899,5.3757341772151905]

val colsims = matrix.columnSimilarities()
val mat1 = colsims.toRowMatrix

import org.apache.spark.mllib.linalg.distributed.MatrixEntry
val transformedRDD = colsims.entries.map{case MatrixEntry(row: Long, col:Long, sim:Double) => ((row,col),sim)}

val rep = transformedRDD.sortBy(_._1).map(x => ((x._1._1,x._1._2),x._2))

var i = -1.0

rep.foreach( x => {
  val sim = x._2
  if (x._1._1 != i) { println
    print(f"$sim%.4f ")
    i = x._1._1
  } else print(f"$sim%.4f ")
})

0.8193 0.8976 0.8392 0.8744 0.6848 0.8174 0.9180 0.9089
0.9443 0.9688 0.9050 0.7782 0.7953 0.8780 0.9532
0.9494 0.9488 0.7754 0.8623 0.9427 0.9841
0.9224 0.7731 0.8407 0.9026 0.9658
0.8292 0.8912 0.9180 0.9622
0.7610 0.7372 0.8037
0.8417 0.8811
0.9741
